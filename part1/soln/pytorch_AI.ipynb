{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to AI in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore the basics of doing classification with PyTorch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we want to demonstrate is that PyTorch can make performing GPU computations very easy. If the GPU is available, using the GPU for matrix multiplication is very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a random matrix of size `N = 10000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = int(1e4)\n",
    "A_cpu = torch.rand(N, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply the matrix by itself and time the result (can use `datetime`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "A_cpu @ A_cpu\n",
    "print(f\"Total time: {datetime.now() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "A_gpu = A_cpu.to(device)\n",
    "start_time = datetime.now()\n",
    "A_gpu @ A_gpu\n",
    "print(f\"Total time: {datetime.now() - start_time}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to deep learning. We first create a classification problem. In this dataset, each spiral arm is of a different class. There are 3 classes, two input values, and 1000 samples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import math\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def plot_scatter(X, y, d=0, auto=False, zoom=1):\n",
    "    X = X.cpu()\n",
    "    y = y.cpu()\n",
    "    plt.scatter(X.numpy()[:, 0], X.numpy()[:, 1], c=y, s=20, cmap=plt.cm.Spectral)\n",
    "    plt.axis('square')\n",
    "    plt.axis(np.array((-1.1, 1.1, -1.1, 1.1)) * zoom)\n",
    "    if auto is True: plt.axis('equal')\n",
    "    # plt.axis('off')\n",
    "\n",
    "    # _m, _c = 0, '.15'\n",
    "    # plt.axvline(0, ymin=_m, color=_c, lw=1, zorder=0)\n",
    "    # plt.axhline(0, xmin=_m, color=_c, lw=1, zorder=0)\n",
    "\n",
    "def plot_model(X, y, model):\n",
    "    model.cpu()\n",
    "    mesh = np.arange(-1.1, 1.1, 0.01)\n",
    "    xx, yy = np.meshgrid(mesh, mesh)\n",
    "    with torch.no_grad():\n",
    "        data = torch.from_numpy(np.vstack((xx.reshape(-1), yy.reshape(-1))).T).float()\n",
    "        Z = model(data).detach()\n",
    "    Z = np.argmax(Z, axis=1).reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.3)\n",
    "    plot_scatter(X, y)\n",
    "\n",
    "plt.rc('figure', figsize=(5,5), dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "N = 1000  # num_samples_per_class\n",
    "D = 2  # dimensions\n",
    "C = 3  # num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and visualise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cpu = torch.zeros(N * C, D)\n",
    "y_cpu = torch.zeros(N * C, dtype=torch.long)\n",
    "for c in range(C):\n",
    "    index = 0\n",
    "    t = torch.linspace(0, 1, N)\n",
    "    inner_var = torch.linspace((2 * math.pi / C) * (c), \n",
    "                               (2 * math.pi / C) * (2 + c), N) + torch.randn(N) * 0.2\n",
    "    \n",
    "    for ix in range(N * c, N * (c + 1)):\n",
    "        X_cpu[ix] = t[index] * torch.FloatTensor((\n",
    "            math.sin(inner_var[index]), math.cos(inner_var[index])\n",
    "        ))\n",
    "        y_cpu[ix] = c\n",
    "        index += 1\n",
    "\n",
    "print(f\"Shapes: X = {tuple(X_cpu.size())}, Y = {tuple(y_cpu.size())} \")\n",
    "plot_scatter(X_cpu, y_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dataset and DataLoader over the data `X_cpu` and `y_cpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(X_cpu, y_cpu)\n",
    "train_loader = DataLoader(dataset, batch_size= 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `train()` below that encapsulates the training procedure for an epoch (single run over the entire training dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, criteria, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criteria(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Train Epoch: {epoch} \\tLoss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see why neural networks are powerful, we will first train a completely linear model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 100\n",
    "\n",
    "class LinearNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "                    nn.Linear(D, H),\n",
    "                    nn.Linear(H,C)\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layers(x)\n",
    "        return output\n",
    "\n",
    "model = LinearNetwork().to(device)\n",
    "\n",
    "learning_rate =  1e-3\n",
    "decay_factor = 1e-5\n",
    "criteria = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay_factor)\n",
    "\n",
    "for epoch in range(200):\n",
    "    train(epoch, model, criteria, train_loader, optimizer, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the loss change? We can plot the model's region of classification for each class by running the cell below. What sort of classification boundaries are produced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(X_cpu, y_cpu, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement a neural network with nonlinear activations (e.g. ReLU) and see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X_cpu, y_cpu)\n",
    "train_loader = DataLoader(dataset, batch_size= 256)\n",
    "\n",
    "H = 100\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "                    nn.Linear(D, H),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(H,C)\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layers(x)\n",
    "        return output\n",
    "\n",
    "model = MultiLayerPerceptron().to(device)\n",
    "\n",
    "learning_rate =  1e-3\n",
    "decay_factor = 1e-5\n",
    "criteria = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay_factor)\n",
    "\n",
    "for epoch in range(200):\n",
    "    train(epoch, model, criteria, train_loader, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "plot_model(X_cpu, y_cpu, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "\n",
    "We now consider creating a Neural network to run on the classic image classification dataset MNIST. We first load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def model_size(model):\n",
    "    n = 0\n",
    "    for p in list(model.parameters()):\n",
    "        n += p.nelement()\n",
    "    return n\n",
    "\n",
    "training_data = datasets.MNIST('./data', train=True, download=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "test_data = datasets.MNIST('./data', train=False, download=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "input_size = 28 * 28    # image dimensions\n",
    "output_size = 10        # num classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display a random sample from the dataset below. The images are of handwritten digits and the goal is to classify the digits in the images correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 5, 1\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = 1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test(model, accuracy_list):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "    return accuracy_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a simple multi-layer perceptron. Hint: Use `nn.Flatten()` to flatten 2D tensor into a 1D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size) -> None:\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten() # flatten 2D tensor into 1D\n",
    "        self.output = nn.LogSoftmax(dim=1)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.layers(x)\n",
    "        out_ = self.output(logits)\n",
    "        return out_\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings \n",
    "n_features = 6 # number of feature maps\n",
    "\n",
    "model = MLP(input_size, n_features, output_size)\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(model_size(model)))\n",
    "\n",
    "\n",
    "accuracy_list = []\n",
    "\n",
    "for epoch in range(0, 2):\n",
    "    train(epoch, model)\n",
    "    accuracy_list = test(model, accuracy_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For image classification, the most standard NN model is called a Convolutional Neural Network (CNN). We provide an implementation of a CNN for MNIST below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_feature, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(n_feature, n_feature, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(n_feature*4*4, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = x.view(-1, self.n_feature*4*4)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings \n",
    "n_features = 6 # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(model_size(model_cnn)))\n",
    "\n",
    "accuracy_list = []\n",
    "for epoch in range(0, 2):\n",
    "    train(epoch, model_cnn)\n",
    "    test(model_cnn, accuracy_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
